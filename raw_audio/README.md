# Raw audio modelling

In raw audio modelling, we predict the next audio sample given a history of previous ones, directly in the time-domain.

In this context, we compare our Seq-U-Net to the very successful and powerful Wavenet model.

## Requirements
### Youtube-DL

To download the dataset, you can use [youtube-dl](https://github.com/ytdl-org/youtube-dl).
Change to the ```dataset``` directory and execute ```download.sh```.
The dataset will be downloaded into ```train``` and ```test``` subfolders.
Note that some of the songs might not be available on Youtube anymore at the time of your download, so your dataset might be a bit smaller as these pieces are skipped during the download.

## Training

The following commands were used to produce the models from the paper:

* Wavenet baseline model:
```
python train.py --cuda --log_dir logs/wavenet --snapshot_dir snapshots/wavenet --model wavenet
```

* Seq-U-Net model:
```
python train.py --cuda --features 180 --levels 11 --depth 2 --log_dir logs/sequnet --snapshot_dir snapshots/sequnet --model sequnet
```

In case your GPU memory runs out, try decreasing the batch size using ```--batch_size``` parameter.
If training is too slow (GPU is starved), increase the number of processes that load data (```--num_workers```). 

Most of the parameters are used to control the architecture of the Seq-U-Net (like ``--depth`` and ``--levels``), so other variants can easily be trained as well.
The Wavenet configuration is hard-coded on the other hand to provide a stable baseline.

## Producing sounds with trained models!

Trained models can be used to produce audio using the ``generate.py`` script.
Instead of training models yourself, you can also [download](https://www.dropbox.com/s/69oh0iwhy153ncd/models.zip?dl=1) the trained Wavenet and Seq-U-Net we used in the paper, and extract the archive so that the ``raw_audio`` folder contains the ``snapshots`` folder from the archive.

As an example, the following command generates a few conditional samples (meaning they aim to continue a real audio sample):

```
python generate.py --cuda --conditional 1 --gen_samples 16000 --features 180 --levels 11 --depth 2 --snapshot_dir snapshots/sequnet --model sequnet --batch_size 2 --num_batches 3
```

* ``--conditional 1`` activates conditional generation mode: real sounds are drawn from the test dataset, and fed into the Seq-U-Net/Wavenet, to continue these sounds. Setting to 0 means only silence will be fed in the beginning, so that the models produce sounds without context/from "scratch"
* ``--gen_samples 16000`` indicates 16000 audio samples should be generated by the model (1 second of audio@16KHz sampling rate)
* ``features``, ``levels``, ``depth`` are set to match the parameters used to train the Seq-U-Net we now want to sample from
* ``--snapshot_dir`` points to the folder containing the model snapshot. The latest checkpoint in this folder is chosen for generation.
* ``--model`` indicates whether to use Seq-U-Net or Wavenet
* ``--batch_size 2 --num_batches 3`` means we use a batch size of 2 for generation, and generate 3 such batches sequentially, obtaining 2*3=6 audio examples at the end

NOTE: Generation only supports a single-GPU setup since we use local attributes of the network modules to cache previous activations, which are not shared properly in a parallel setup.

For more detailed information on the command line arguments (including more settings like temperature), please refer to the argument parser code in ``generate.py``.